# [Understanding LSTM Networks - colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

## Summary
The article is devoted to a booming Neural Networks architechture called LSTM.

The author illustrates how in the last few years, there have been incredible success applying RNNs to a variety of problems: speech recognition, language modeling, translation, image captioning...

The researher demonstrates The Problem of Long-Term Dependencies in classical RNNs. And LSTMs are explicitly designed to avoid this problem.

The author shows fundamental parts of LSTM: chain of repeating modules, pointwise operators, gates and the Core Idea Behind LSTMs: a conveyor belt carrying the cell state.

Then author demonstrates Step-by-Step LSTM Walk Through.
In the end author gives us some information on the most advanced Variants of LSTM studied in recent papers, especially those ones which uses attention.

The author concludes that this very special kind of neural network works, for many tasks, much much better than the standard version. Almost all exciting results based on recurrent neural networks are achieved with LSTM.

## New Vocab
- recúrrent (not reccurential)
- shortcoming - flaw / недостаток
- reasoning - thinking / рассуждения /логический ход мысли
- address this issue - solve this problem
- to persist - удерживаться / продолжать существовать /  оставаться
- multiple - мАлтипл
- successor - преемник
- one of the appeals of - одна из привлекательных особенностей
- from further back - c предыдущих шагов / с начала / из первых фраз
- refined - purified / clarified
- tremendously - extremely / чрезвычайно
- pointwise - the pointwise product of two functions
- tanh - hyperbolic tangent (танжент) function
- to conjugate - different forms of a verb / спрягать
- intimidating - scarry
- approachable - friendly, welcoming
- peephole - глазок (дверной)
- increasingly - more and more

## Paragraph to read
Any - TODO?
